{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Train YOLOv8 Large (ULTRA Accuracy) on 3 Combined Datasets\n",
                "\n",
                "This is the **Corrected Training Script** with your specific datasets.\n",
                "1.  **`pkdarabi/helmet`** (Base)\n",
                "2.  **`aneesarom/rider-with-helmet...`**\n",
                "3.  **`rhamzanul/smart-helmet-detection`**\n",
                "\n",
                "**Model**: Uses **YOLOv8l** (Large).\n",
                "\n",
                "### ⚠️ CRITICAL STEP ⚠️\n",
                "**You MUST accept rules for ALL 3 datasets:**\n",
                "1.  [pkdarabi/helmet](https://www.kaggle.com/datasets/pkdarabi/helmet)\n",
                "2.  [aneesarom/rider-with-helmet-without-helmet-number-plate](https://www.kaggle.com/datasets/aneesarom/rider-with-helmet-without-helmet-number-plate)\n",
                "3.  [rhamzanul/smart-helmet-detection-using-yolo-v8](https://www.kaggle.com/datasets/rhamzanul/smart-helmet-detection-using-yolo-v8)\n",
                "\n",
                "### Steps\n",
                "1.  **Factory Reset**: Runtime -> Disconnect and Delete Runtime.\n",
                "2.  **Upload `kaggle.json`** when prompted.\n",
                "3.  **Run All**."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install ultralytics kaggle pyyaml"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "from google.colab import files\n",
                "import shutil\n",
                "\n",
                "# 0. CLEANUP \n",
                "print(\"Cleaning up old runs...\")\n",
                "dirs_to_clean = ['ds1', 'ds2', 'ds3', 'final_dataset', 'dataset', 'runs']\n",
                "for d in dirs_to_clean:\n",
                "    if os.path.exists(d):\n",
                "        shutil.rmtree(d)\n",
                "\n",
                "# 1. Upload kaggle.json\n",
                "print(\"Please upload your 'kaggle.json' file:\")\n",
                "if not os.path.exists('/root/.kaggle/kaggle.json') and not os.path.exists('kaggle.json'):\n",
                "    uploaded = files.upload()\n",
                "    for fn in uploaded.keys():\n",
                "        print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
                "            name=fn, length=len(uploaded[fn])))\n",
                "    !mkdir -p ~/.kaggle\n",
                "    !mv kaggle.json ~/.kaggle/\n",
                "    !chmod 600 ~/.kaggle/kaggle.json\n",
                "    print(\"Kaggle API configured!\")\n",
                "else:\n",
                "    if os.path.exists('kaggle.json'):\n",
                "        !mkdir -p ~/.kaggle\n",
                "        !mv kaggle.json ~/.kaggle/\n",
                "        !chmod 600 ~/.kaggle/kaggle.json\n",
                "    print(\"Kaggle API already configured.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. Download and Merge 3 Datasets\n",
                "import yaml\n",
                "import glob\n",
                "import shutil\n",
                "from pathlib import Path\n",
                "\n",
                "def download_dataset(slug, folder_name):\n",
                "    print(f\"Downloading {slug}...\")\n",
                "    !kaggle datasets download -d {slug}\n",
                "    zip_name = slug.split('/')[-1] + \".zip\"\n",
                "    !unzip -qo {zip_name} -d {folder_name}\n",
                "    !rm {zip_name}\n",
                "\n",
                "# YOUR CORRECTED LIST\n",
                "datasets = [\n",
                "    {'slug': 'pkdarabi/helmet', 'folder': 'ds1'},\n",
                "    {'slug': 'aneesarom/rider-with-helmet-without-helmet-number-plate', 'folder': 'ds2'},\n",
                "    {'slug': 'rhamzanul/smart-helmet-detection-using-yolo-v8', 'folder': 'ds3'}\n",
                "]\n",
                "\n",
                "for ds in datasets:\n",
                "    try:\n",
                "        download_dataset(ds['slug'], ds['folder'])\n",
                "        print(f\"Successfully downloaded {ds['slug']}\")\n",
                "    except Exception as e:\n",
                "        print(f\"Error downloading {ds['slug']}: {e}\")\n",
                "        os.makedirs(ds['folder'], exist_ok=True)\n",
                "\n",
                "# Merge Logic\n",
                "print(\"Merging 3 Datasets...\")\n",
                "\n",
                "# Final Classes: 0: Helmet, 1: No Helmet, 2: Plate\n",
                "target_classes = ['Helmet', 'No Helmet', 'Plate']\n",
                "final_dataset_path = 'final_dataset'\n",
                "\n",
                "for split in ['train', 'valid', 'test']:\n",
                "    os.makedirs(f\"{final_dataset_path}/{split}/images\", exist_ok=True)\n",
                "    os.makedirs(f\"{final_dataset_path}/{split}/labels\", exist_ok=True)\n",
                "\n",
                "def match_class(name):\n",
                "    name = name.lower()\n",
                "    if 'no' in name or 'without' in name or 'head' in name:\n",
                "        return 1 # No Helmet\n",
                "    elif 'helmet' in name and 'no' not in name:\n",
                "        return 0 # Helmet\n",
                "    elif 'plate' in name or 'license' in name or 'number' in name:\n",
                "        return 2 # Plate\n",
                "    return -1 # Ignore others\n",
                "\n",
                "for ds in datasets:\n",
                "    yaml_files = glob.glob(f\"{ds['folder']}/**/data.yaml\", recursive=True)\n",
                "    if not yaml_files:\n",
                "        print(f\"Skipping {ds['folder']}: No data.yaml found\")\n",
                "        continue\n",
                "        \n",
                "    with open(yaml_files[0], 'r') as f:\n",
                "        data_config = yaml.safe_load(f)\n",
                "    \n",
                "    names = data_config.get('names', [])\n",
                "    print(f\"Dataset {ds['folder']} classes: {names}\")\n",
                "    \n",
                "    id_map = {}\n",
                "    for i, name in enumerate(names):\n",
                "        new_id = match_class(name)\n",
                "        if new_id != -1:\n",
                "            id_map[i] = new_id\n",
                "\n",
                "    for split in ['train', 'valid', 'test']:\n",
                "        split_images = glob.glob(f\"{ds['folder']}/**/{split}/images/*.*\", recursive=True)\n",
                "        if not split_images and split == 'valid':\n",
                "             split_images = glob.glob(f\"{ds['folder']}/**/val/images/*.*\", recursive=True)\n",
                "        \n",
                "        print(f\"Processing {len(split_images)} images for {ds['folder']} - {split}\")\n",
                "        \n",
                "        for img_path in split_images:\n",
                "            lbl_path = img_path.replace('images', 'labels').rsplit('.', 1)[0] + \".txt\"\n",
                "            if os.path.exists(lbl_path):\n",
                "                new_lines = []\n",
                "                with open(lbl_path, 'r') as f:\n",
                "                    lines = f.readlines()\n",
                "                    for line in lines:\n",
                "                        parts = line.strip().split()\n",
                "                        if len(parts) >= 5:\n",
                "                            cls = int(parts[0])\n",
                "                            if cls in id_map:\n",
                "                                new_cls = id_map[cls]\n",
                "                                new_lines.append(f\"{new_cls} \" + \" \".join(parts[1:]))\n",
                "                \n",
                "                if new_lines:\n",
                "                    basename = f\"{ds['folder']}_{os.path.basename(lbl_path)}\"\n",
                "                    with open(f\"{final_dataset_path}/{split}/labels/{basename}\", 'w') as f:\n",
                "                        f.write(\"\\n\".join(new_lines))\n",
                "                    img_basename = f\"{ds['folder']}_{os.path.basename(img_path)}\"\n",
                "                    shutil.copy(img_path, f\"{final_dataset_path}/{split}/images/{img_basename}\")\n",
                "\n",
                "# Create final data.yaml\n",
                "final_yaml = {\n",
                "    'train': f\"/content/{final_dataset_path}/train/images\",\n",
                "    'val': f\"/content/{final_dataset_path}/valid/images\",\n",
                "    'nc': 3,\n",
                "    'names': target_classes\n",
                "}\n",
                "\n",
                "with open(f\"{final_dataset_path}/data.yaml\", 'w') as f:\n",
                "    yaml.dump(final_yaml, f)\n",
                "\n",
                "print(\"Merge Complete! Mega-Dataset created.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4. Train Model\n",
                "from ultralytics import YOLO\n",
                "\n",
                "# Load Large model for MAXIMUM Accuracy\n",
                "model = YOLO('yolov8l.pt') \n",
                "\n",
                "# Train\n",
                "print(\"Starting training on merged dataset (Large Model)...\")\n",
                "print(\"Training for 200 Epochs for Absolute Best Results...\")\n",
                "model.train(data=f\"{final_dataset_path}/data.yaml\", epochs=50, imgsz=640, project='runs/detect', name='train')\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 5. Export\n",
                "from google.colab import files\n",
                "import glob\n",
                "\n",
                "weights_files = glob.glob('runs/detect/train/weights/best.pt', recursive=True)\n",
                "\n",
                "if weights_files:\n",
                "    print(f\"Found model at: {weights_files[0]}\")\n",
                "    print(\"Downloading... This is a larger file (~80-100MB), please wait.\")\n",
                "    files.download(weights_files[0])\n",
                "else:\n",
                "    print(\"Model not found. Please checks 'runs/detect/train' folder manually.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}